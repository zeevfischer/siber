{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bad7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dct test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01710f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dct and try to brighten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f014bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io, color, img_as_ubyte\n",
    "from scipy.fftpack import dct, idct\n",
    "\n",
    "# Load an example color image\n",
    "image_path = r\"C:\\Users\\USER\\Desktop\\siber_test\\photos\\P1110564.JPG\"\n",
    "image = io.imread(image_path)\n",
    "\n",
    "block_size = 2\n",
    "# Apply DCT to each color channel\n",
    "dct_features = np.zeros_like(image, dtype=np.float64)\n",
    "\n",
    "for c in range(image.shape[2]):\n",
    "    channel = image[:, :, c]\n",
    "    \n",
    "    for i in range(0, channel.shape[0], block_size):\n",
    "        for j in range(0, channel.shape[1], block_size):\n",
    "            block = channel[i:i + block_size, j:j + block_size]\n",
    "            dct_block = dct(dct(block.T, norm='ortho').T, norm='ortho')\n",
    "            dct_features[i:i + block_size, j:j + block_size, c] = dct_block\n",
    "\n",
    "# Normalize the DCT coefficients differently\n",
    "dct_features_normalized = (dct_features - np.min(dct_features)) / (np.max(dct_features) - np.min(dct_features))\n",
    "\n",
    "# io.imsave('dct_transformed_image.png', img_as_ubyte(dct_features_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88914ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brighten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f917da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "def adjust_brightness(image, brightness_factor):\n",
    "    pil_image = Image.fromarray(img_as_ubyte(image))\n",
    "    # Apply brightness adjustment\n",
    "    enhancer = ImageEnhance.Brightness(pil_image)\n",
    "    image_brightness_adjusted = enhancer.enhance(brightness_factor)\n",
    "    \n",
    "    # Convert back to numpy array\n",
    "    adjusted_array = np.asarray(image_brightness_adjusted)\n",
    "    \n",
    "    return adjusted_array\n",
    "\n",
    "# Example usage\n",
    "adjusted_image = adjust_brightness(dct_features_normalized, 1.3)\n",
    "\n",
    "# Save the result\n",
    "io.imsave('brightness_adjusted_image.png', img_as_ubyte(adjusted_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the dct and original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c951a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from skimage import io, color, util, img_as_ubyte\n",
    "# from scipy.fftpack import dct, idct\n",
    "\n",
    "# # Load an example color image\n",
    "# image_path = r\"C:\\Users\\USER\\Desktop\\siber_test\\photos\\P1110564.JPG\"\n",
    "# image = io.imread(image_path)\n",
    "\n",
    "# block_size = 2\n",
    "# # Apply DCT to each color channel\n",
    "# dct_features = np.zeros_like(image, dtype=np.float64)\n",
    "\n",
    "# for c in range(image.shape[2]):\n",
    "#     channel = image[:, :, c]\n",
    "    \n",
    "#     for i in range(0, channel.shape[0], block_size):\n",
    "#         for j in range(0, channel.shape[1], block_size):\n",
    "#             block = channel[i:i + block_size, j:j + block_size]\n",
    "#             dct_block = dct(dct(block.T, norm='ortho').T, norm='ortho')\n",
    "#             dct_features[i:i + block_size, j:j + block_size, c] = dct_block\n",
    "            \n",
    "# # Combine dct_features with the original image\n",
    "# combined_image = image + (dct_features * 0.1)\n",
    "\n",
    "# # Normalize the combined image to be in the range [0, 1]\n",
    "# combined_image_normalized = (combined_image - np.min(combined_image)) / (np.max(combined_image) - np.min(combined_image))\n",
    "\n",
    "# # Convert the normalized combined image to uint8\n",
    "# combined_image_uint8 = img_as_ubyte(combined_image_normalized)\n",
    "\n",
    "# io.imsave('combined_image123.png', img_as_ubyte(combined_image_uint8))\n",
    "\n",
    "\n",
    "# # # Normalize the DCT coefficients differently\n",
    "# # dct_features_normalized = (dct_features - np.min(dct_features)) / (np.max(dct_features) - np.min(dct_features))\n",
    "# # # ________________________________________________________________________\n",
    "# # # Reconstruct the image using the inverse DCT (IDCT)\n",
    "# # # reconstructed_image = np.zeros_like(image, dtype=np.float64)\n",
    "\n",
    "# # # # # Reconstruct the image using the inverse DCT (IDCT)\n",
    "# # # reconstructed_image = np.zeros_like(image)\n",
    "# # # ________________________________________________________________________\n",
    "\n",
    "# # # for c in range(image.shape[2]):\n",
    "# # #     for i in range(0, channel.shape[0], block_size):\n",
    "# # #         for j in range(0, channel.shape[1], block_size):\n",
    "# # #             dct_block = dct_features_normalized[i:i + block_size, j:j + block_size, c]\n",
    "# # #             block = idct(idct(dct_block.T, norm='ortho').T, norm='ortho')\n",
    "# # #             reconstructed_image[i:i + block_size, j:j + block_size, c] = block\n",
    "# # # ________________________________________________________________________\n",
    "# # # Normalize the reconstructed image differently\n",
    "# # # reconstructed_image_normalized = (reconstructed_image - np.min(reconstructed_image)) / (np.max(reconstructed_image) - np.min(reconstructed_image))\n",
    "\n",
    "# # # Clip the values to be within the valid range [0, 1]\n",
    "# # # reconstructed_image_normalized = np.clip(reconstructed_image_normalized, 0, 1)\n",
    "# # # ________________________________________________________________________\n",
    "\n",
    "# # # Save the original, DCT-transformed, and reconstructed images\n",
    "# # # io.imsave('original_image.png', img_as_ubyte(image))\n",
    "# # io.imsave('dct_transformed_image.png', img_as_ubyte(dct_features_normalized))\n",
    "# # # io.imsave('reconstructed_image.png', img_as_ubyte(reconstructed_image_normalized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d32252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8132271e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from skimage import io, color, img_as_ubyte\n",
    "# from scipy.fftpack import dct, idct\n",
    "\n",
    "# # Load an example color image\n",
    "# image_path = r\"C:\\Users\\USER\\Desktop\\siber_test\\photos\\P1110564.JPG\"\n",
    "# image = io.imread(image_path)\n",
    "\n",
    "# block_size = 8\n",
    "# # Apply DCT to each color channel\n",
    "# dct_features = np.zeros_like(image, dtype=np.float64)\n",
    "\n",
    "# for c in range(image.shape[2]):\n",
    "#     channel = image[:, :, c]\n",
    "    \n",
    "#     for i in range(0, channel.shape[0], block_size):\n",
    "#         for j in range(0, channel.shape[1], block_size):\n",
    "#             block = channel[i:i + block_size, j:j + block_size]\n",
    "#             dct_block = dct(dct(block.T, norm='ortho').T, norm='ortho')\n",
    "#             dct_features[i:i + block_size, j:j + block_size, c] = dct_block\n",
    "\n",
    "# # Normalize the DCT coefficients differently\n",
    "# dct_features_normalized = (dct_features - np.min(dct_features)) / (np.max(dct_features) - np.min(dct_features))\n",
    "\n",
    "# # Reconstruct the image using the inverse DCT (IDCT)\n",
    "# reconstructed_image = np.zeros_like(image, dtype=np.float64)\n",
    "\n",
    "# for c in range(image.shape[2]):\n",
    "#     for i in range(0, channel.shape[0], block_size):\n",
    "#         for j in range(0, channel.shape[1], block_size):\n",
    "#             dct_block = dct_features_normalized[i:i + block_size, j:j + block_size, c]\n",
    "#             block = idct(idct(dct_block.T, norm='ortho').T, norm='ortho')\n",
    "#             reconstructed_image[i:i + block_size, j:j + block_size, c] = block\n",
    "\n",
    "# # Normalize the reconstructed image differently\n",
    "# reconstructed_image_normalized = (reconstructed_image - np.min(reconstructed_image)) / (np.max(reconstructed_image) - np.min(reconstructed_image))\n",
    "\n",
    "# # Save the original, DCT-transformed, reconstructed, and combined images\n",
    "# io.imsave('original_image.png', img_as_ubyte(image))\n",
    "# io.imsave('dct_transformed_image.png', img_as_ubyte(dct_features_normalized))\n",
    "# io.imsave('reconstructed_image.png', img_as_ubyte(reconstructed_image_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code tecnecly works but takes a milion year to run and did not finish yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "71e7d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# import torch\n",
    "\n",
    "# # Load the YOLOv5 model\n",
    "# model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n",
    "\n",
    "# # Load and preprocess the image\n",
    "# img_path = r\"C:\\Users\\USER\\Desktop\\siber_test\\photos\\P1110564.JPG\"  # Change path as needed\n",
    "# img = cv2.imread(img_path)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# img = cv2.resize(img, (640, 640))\n",
    "\n",
    "# # Convert image to a PyTorch tensor\n",
    "# img_tensor = torch.from_numpy(img.transpose((2, 0, 1))).float() / 255.0\n",
    "# img_tensor = img_tensor.unsqueeze(0)\n",
    "# img_tensor.requires_grad_(True)\n",
    "\n",
    "# # Perform a forward pass to obtain the predicted output\n",
    "# model.eval()\n",
    "# predictions = model(img_tensor)\n",
    "\n",
    "# # Extract relevant information from predictions\n",
    "# detected_objects = predictions[0][:, -1].int()  # Class labels\n",
    "# num_objects = len(detected_objects)\n",
    "\n",
    "# # Placeholder target labels for multiple objects\n",
    "# # Assuming format: [class, x_center, y_center, width, height]\n",
    "# target_labels = torch.rand((num_objects, 5))\n",
    "\n",
    "# # Fill target_labels with relevant information from predictions\n",
    "# for i in range(num_objects):\n",
    "#     target_labels[i, 0] = detected_objects[i].item()  # class label\n",
    "#     target_labels[i, 1:5] = predictions[0][i, 1:5]  # bounding box coordinates\n",
    "\n",
    "# # Use Mean Squared Error Loss for bounding box regression\n",
    "# mse_loss = torch.nn.MSELoss()\n",
    "# loss = mse_loss(predictions[0][:, 1:5], target_labels[:, 1:5])\n",
    "# loss.requires_grad_(True)\n",
    "# # Calculate gradient of loss with respect to input image\n",
    "# loss.backward()\n",
    "\n",
    "# # Estimate gradients using finite differences\n",
    "# epsilon = 1e-6\n",
    "# gradients_manual = torch.zeros_like(img_tensor)\n",
    "# for channel in range(3):  # Assuming RGB image\n",
    "#     for i in range(img_tensor.shape[2]):\n",
    "#         for j in range(img_tensor.shape[3]):\n",
    "#             img_perturbed = img_tensor.clone()\n",
    "#             img_perturbed[0, channel, i, j] += epsilon\n",
    "#             predictions_perturbed = model(img_perturbed)\n",
    "#             loss_perturbed = mse_loss(predictions_perturbed[0][:, 1:5], target_labels[:, 1:5])\n",
    "#             gradients_manual[0, channel, i, j] = (loss_perturbed - loss) / epsilon\n",
    "\n",
    "# # Combine the original image and distorted image with manual gradients\n",
    "# distorted_img_manual = img + 0.1 * gradients_manual.numpy()\n",
    "# distorted_img_manual = np.clip(distorted_img_manual, 0, 255).astype(np.uint8)\n",
    "\n",
    "# # Save the distorted image\n",
    "# cv2.imwrite(\"distorted_image_manual.jpg\", cv2.cvtColor(distorted_img_manual, cv2.COLOR_RGB2BGR))\n",
    "# print(\"Distorted image saved as distorted_image_manual.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a5279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
